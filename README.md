 Advanced Time Series Forecasting with Deep Learning & Explainability

This project implements an end-to-end advanced time series forecasting pipeline using deep learning, Bayesian hyperparameter optimization, baseline statistical models, and model explainability techniques. The focus is not only on prediction accuracy, but also on robust evaluation and interpretability for multivariate, non-stationary time series data.

ğŸš€ Project Objectives

Generate a multivariate, non-stationary synthetic time series dataset

Train a sequence-to-sequence LSTM model for multi-step forecasting

Apply rolling-window style evaluation and robust performance metrics

Optimize hyperparameters using Optuna (Bayesian Optimization)

Compare deep learning performance against a baseline ARIMA model

Interpret model predictions using SHAP explainability

ğŸ§  Key Features

âœ” 1200+ observations with 5 input features

âœ” Non-stationary trend, seasonality, and volatility

âœ” Multi-horizon forecasting (5-step ahead)

âœ” LSTM-based deep learning model (PyTorch)

âœ” Optuna-based hyperparameter tuning

âœ” ARIMA baseline comparison

âœ” SHAP explainability adapted for time-series models

âœ” RMSE & MAE evaluation metrics

ğŸ“‚ Project Structure
â”œâ”€â”€ advanced_time_series_forecasting.py
â”œâ”€â”€ README.md

ğŸ§ª Dataset Description

The dataset is synthetically generated to simulate real-world time series characteristics:

Feature	Description
trend	Linear non-stationary trend
seasonality	Sinusoidal seasonal component
volatility	Increasing variance over time
feature4	Cosine-based periodic signal
feature5	Random noise feature
target	Weighted combination of features + noise

Total observations: 1200
Sequence length: 30 time steps
Forecast horizon: 5 future steps

ğŸ—ï¸ Model Architecture
ğŸ”¹ Deep Learning Model

Model: LSTM (Sequence-to-Sequence)

Framework: PyTorch

Loss Function: Mean Squared Error (MSE)

Optimizer: Adam

Output: Multi-step forecast (5 steps)

ğŸ”¹ Baseline Model

ARIMA (2,1,2) using statsmodels

âš™ï¸ Hyperparameter Optimization

Hyperparameters are tuned using Optuna, including:

Number of LSTM hidden units

Number of LSTM layers

Learning rate

Optimization Objective:
Minimize RMSE on the test set

ğŸ“Š Evaluation Metrics

RMSE (Root Mean Squared Error)

MAE (Mean Absolute Error)

Performance is reported for:

Optimized LSTM model

Baseline ARIMA model

ğŸ” Explainability with SHAP

Since LSTM models require 3D input (samples Ã— time Ã— features) and SHAP supports 2D inputs, the time dimension is flattened for explainability.

Explainability Strategy:

Flatten time-series input for SHAP compatibility

Reshape data back to 3D inside the prediction wrapper

Use SHAP KernelExplainer to compute feature-time importance

This approach provides insight into which features and time steps most influence predictions.

ğŸ“¦ Installation

Install the required dependencies:

pip install numpy pandas torch scikit-learn optuna statsmodels shap

â–¶ï¸ How to Run
python advanced_time_series_forecasting.py

ğŸ“Œ Sample Output
Deep Learning RMSE: 0.0XXX
Deep Learning MAE: 0.0XXX
ARIMA RMSE: 0.0XXX
SHAP explainability completed successfully

ğŸ“ Academic Relevance

This project aligns with advanced coursework in:

Time Series Analysis

Deep Learning

Model Explainability (XAI)

Bayesian Optimization

It is suitable for:

University assignments

Capstone projects

Data science portfolios

Research prototypes

ğŸ“œ License

This project is for educational and academic use.

ğŸ™Œ Acknowledgements

PyTorch

Optuna

SHAP

Statsmodels

Scikit-learn
# Final-Project
